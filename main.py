import sys
import os
os.environ["HF_HOME"] = "/cache/hf"
os.environ["TRANSFORMERS_CACHE"] = "/cache/hf"
os.environ["HF_HUB_CACHE"] = "/cache/hf"

# å¼ºåˆ¶å°† HuggingFace çš„ç«¯ç‚¹æŒ‡å‘å›½å†…é•œåƒç«™
#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import argparse
import json
import mindspore as ms
import mindnlp
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from threading import Thread
import time
# å¼•å…¥åˆšæ‰å†™çš„ Graph RAG ç±»
from graph_rag import KnowledgeGraphRAG

            
def should_retrieve(self, query, history):
    """
    ä½¿ç”¨ LLM åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡ŒçŸ¥è¯†å›¾è°±æ£€ç´¢ã€‚
    è¾“å…¥åŒ…æ‹¬ query + åŽ†å²ï¼Œä½¿æ¨¡åž‹çŸ¥é“å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‚
    """
    
    # æž„é€ å¯è¯»çš„ç®€çŸ­åŽ†å²
    history_text = ""
    for q, a in history[-3:]:
        history_text += f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}\n"

    decision_prompt = [
        {
            "role": "system",
            "content":
            "ä½ æ˜¯ä¸€ä¸ªåˆ¤æ–­æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢çš„åˆ†ç±»å™¨ã€‚\n"
            "å½“ç”¨æˆ·é—®çš„é—®é¢˜ä¸Žæ—…æ¸¸æ™¯ç‚¹ã€ç¥¨ä»·ã€åœ°å€ã€ç”µè¯ã€å¼€æ”¾æ—¶é—´ã€è·¯çº¿ã€é™„è¿‘ã€åŽ†å²æœ‰å…³æ—¶ â†’ è¾“å‡º YESã€‚\n"
            "å³ä½¿å½“å‰å¥å­æ˜¯æ¨¡ç³Šé—®å¥ï¼ˆå¦‚â€œè¿˜æœ‰å—ï¼Ÿâ€ã€â€œé‚£å‘¢ï¼Ÿâ€ã€â€œè¦â€ï¼‰ï¼Œåªè¦æ ¹æ®â€ä¸Šä¸‹æ–‡â€œåˆ¤æ–­ä¸Žä¹‹å‰æ—…æ¸¸é—®é¢˜ç›¸å…³ â†’ è¾“å‡º YESã€‚\n"
            "è‹¥é—®é¢˜æ˜¯é—²èŠã€ä¸»è§‚æé—®ã€è‡ªæˆ‘ä»‹ç»æˆ–æ— å…³è¯­å¥ â†’ è¾“å‡º NOã€‚\n"
            "åªè¾“å‡º YES æˆ– NOï¼Œä¸è¦è§£é‡Šã€‚\n"
        },
        {
            "role": "user",
            "content":
            f"ã€å¯¹è¯åŽ†å²ã€‘\n{history_text}\n"
            f"ã€å½“å‰ç”¨æˆ·é—®é¢˜ã€‘\n{query}\n"
            "è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢ã€‚"
        }
    ]

    # Tokenize
    inputs = self.tokenizer.apply_chat_template(
        decision_prompt,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="ms"
    )

    # ç§»åŠ¨åˆ° NPU
    if 'Ascend' in ms.context.get_context('device_target'):
        inputs = {k: v.to("npu") for k, v in inputs.items()}

    output = self.model.generate(
        **inputs,
        max_new_tokens=4,
        do_sample=False
    )

    text = self.tokenizer.decode(output[0], skip_special_tokens=True).strip().upper()

    return "YES" in text


class GraphRAGServer:
    def __init__(self, model_path, lora_path, knowledge_path):
        """
        åˆå§‹åŒ– Graph RAG æœåŠ¡å™¨
        """
        self.knowledge_path = knowledge_path
        
        # 1. åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
        self.kg = KnowledgeGraphRAG(knowledge_path)
        
        
        # 2. åŠ è½½æ¨¡åž‹å’Œ Tokenizer
        print("æ­£åœ¨åŠ è½½æ¨¡åž‹...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                'Qwen/Qwen2.5-7B-Instruct'  , 
                use_fast=False, 
                mirror='modelscope', 
                trust_remote_code=True
            )

            # MindSpore çŽ¯å¢ƒä¸‹åŠ è½½æ¨¡åž‹

            self.model = AutoModelForCausalLM.from_pretrained(
                'Qwen/Qwen2.5-7B-Instruct' , 
                # 'Qwen/Qwen2.5-7B-Instruct' ,
                ms_dtype=ms.bfloat16, # æˆ–è€… ms.bfloat16
                mirror='modelscope', 
                device_map=0,
                trust_remote_code=True
            )

            # å¦‚æžœæœ‰ LoRA (PeftModel)ï¼Œåœ¨è¿™é‡ŒåŠ è½½
            if lora_path and os.path.exists(lora_path):
                from peft import PeftModel
                print(f"æ­£åœ¨åŠ è½½ LoRA æƒé‡: {lora_path}")
                self.model = PeftModel.from_pretrained(self.model, model_id=lora_path)

            print("æ¨¡åž‹åŠ è½½å®Œæˆï¼")

        except Exception as e:
            print(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
            

    def generate_response(self, query, history=[]):
        """
        ç”Ÿæˆå›žå¤æµç¨‹
        """
    
        try:
            need_rag = should_retrieve_llm(self.model, self.tokenizer, query, history)
        except:
            need_rag = True
            
        system_prompt = (
                "ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒ—äº¬æ—…æ¸¸åŠ©æ‰‹ã€‚\n"
                "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„çŸ¥è¯†æˆ–å¸¸è¯†å›žç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
                "ç¦æ­¢è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚"
            )

         # ----------------- æž„å»º messages -----------------
        messages = [{"role": "system", "content": system_prompt}]

        # åŽ†å²å¯¹è¯
        for q, a in history:
            messages.append({"role": "user", "content": q})
            messages.append({"role": "assistant", "content": a})

        # Step 2: å¦‚æžœéœ€è¦æ£€ç´¢
        if need_rag:
            # print("â†’ æœ¬è½®éœ€è¦æ£€ç´¢å›¾è°±")

            knowledge = self.kg.retrieve(query)
            # print('æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼š',knowledge)
            
            if knowledge:
                # RAG çŸ¥è¯†æ”¾åœ¨ â€œassistantâ€ è§’è‰²ï¼Œæƒé‡æœ€é«˜
                messages.append({
                    "role": "assistant",
                    "content": f"ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æžœã€‘\n{knowledge}"
                })
            else:
                messages.append({
                    "role": "assistant",
                    "content": "ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æžœä¸ºç©ºã€‘"
                })

        else:
            print("â†’ å±žäºŽé—²èŠï¼Œä¸æ£€ç´¢çŸ¥è¯†åº“")

        # ç”¨æˆ·å½“å‰é—®é¢˜
        messages.append({"role": "user", "content": query})

      
        from transformers import TextIteratorStreamer # ç¡®ä¿è¿™ä¸ªå¯¼å…¥è¯­å¥åœ¨æ–‡ä»¶é¡¶éƒ¨
        
        # 1. åœ¨ try å—å¤–éƒ¨å®šä¹‰ streamer
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        # ----------------------------------------------------
        
        # Step 3: æ¨¡åž‹æŽ¨ç†
        try:
            # 1. Tokenizerç”Ÿæˆè¾“å…¥
            input_ids_dict = self.tokenizer.apply_chat_template( # é‡å‘½åå˜é‡ä¸º input_ids_dict
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="ms", # è¿”å›ž MindSpore Tensor
                return_dict=True
            )

            # --- ðŸŒŸ å…³é”®ä¿®æ­£ï¼šå°†æ‰€æœ‰è¾“å…¥å¼ é‡ç§»åŠ¨åˆ° NPU/Ascend è®¾å¤‡ ---
            # é»˜è®¤æ¨¡åž‹æ˜¯åœ¨ NPU ä¸Šï¼ˆdevice_map=0æˆ–Ascendä¸Šä¸‹æ–‡ï¼‰
            if 'Ascend' in ms.context.get_context('device_target'):
                # éåŽ†å­—å…¸ä¸­çš„æ‰€æœ‰å¼ é‡ï¼Œå¹¶ä½¿ç”¨ .to('npu') ç§»åŠ¨å®ƒä»¬
                input_ids = {k: v.to('npu') for k, v in input_ids_dict.items()}
            else:
                # å¦‚æžœä¸æ˜¯ Ascend çŽ¯å¢ƒï¼Œåˆ™ä¿æŒåŽŸæ ·æˆ–ç§»åŠ¨åˆ° 'cpu'
                input_ids = input_ids_dict
        
            
            generation_kwargs = dict(
                input_ids,
                streamer=streamer,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
            )
            
            # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            generated_text = ""
            for new_text in streamer:
                print(new_text, end="", flush=True)
                generated_text += new_text
            print() # æ¢è¡Œ
            
            return generated_text
            
        except Exception as e:
            print(f"ç”Ÿæˆå‡ºé”™: {e}")
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚"

def main():
    # æ¨¡æ‹Ÿå‚æ•°ï¼Œå®žé™…ä½¿ç”¨å¯æ›¿æ¢ä¸º argparse
    # MODEL_PATH = 'Qwen/Qwen3-8B'
    MODEL_PATH ='Qwen/Qwen2.5-7B-Instruct'
    LORA_PATH = './qwen2.5-7B_lora_output/checkpoint-800' 
    KNOWLEDGE_FILE = 'train.json' # æ‚¨çš„æ•°æ®é›†
    
    if not os.path.exists(KNOWLEDGE_FILE):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°çŸ¥è¯†åº“æ–‡ä»¶ {KNOWLEDGE_FILE}")
        return

    server = GraphRAGServer(MODEL_PATH, LORA_PATH, KNOWLEDGE_FILE)
    
    history = []
    
    print("="*50)
    print("Graph RAG æ—…æ¸¸åŠ©æ‰‹å·²å¯åŠ¨ (è¾“å…¥ quit é€€å‡º)")
    print("="*50)
    
    while True:
        query = input("\nç”¨æˆ·: ").strip()
        if query.lower() in ['quit', 'exit']:
            break
        if not query:
            continue
            
        print("åŠ©æ‰‹: ", end="")
        # print("hisotry:",history)
        response = server.generate_response(query, history)
      
        # æ›´æ–°ç®€æ˜“åŽ†å²
        history.append((query, response))
        if len(history) > 3:
            history.pop(0)

if __name__ == "__main__":
    main()