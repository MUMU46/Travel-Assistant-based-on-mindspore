import mindnlp
import os
import sys
from datasets import Dataset
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig
import mindspore as ms
from mindnlp.peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
from mindspore.communication import init, get_rank, get_group_size

def train():
    # ================= 1. åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ– =================
    try:
        ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend")
        init()
        rank_id = get_rank()
        rank_size = get_group_size()
        print(f"ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ: Rank {rank_id}/{rank_size}")
    except:
        rank_id = 0
        rank_size = 1
        print("âš ï¸ æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒï¼Œå¯åŠ¨å•å¡æ¨¡å¼")

    # ================= 2. é…ç½®å‚æ•° =================
    MODEL_PATH = "Qwen/Qwen2.5-7B-Instruct"
    DATA_FILE = "./data_preprocess/train_data.jsonl"
    OUTPUT_DIR = "./qwen2.5-7B_lora_output"
    CACHE_DIR = "/cache/hf" 
    
    # ================= 3. åŠ è½½æ¨¡å‹ä¸Tokenizer =================
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, mirror='modelscope', trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

        # ================= 4. æ¨¡å‹åŠ è½½ï¼ˆ4bit + BF16ï¼‰ =================
   
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ (BF16)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        mirror='modelscope',
        cache_dir=CACHE_DIR,  
        ms_dtype=ms.bfloat16,    # Ascend 910B æ¨èä½¿ç”¨ BF16
        trust_remote_code=True,
        attn_implementation="eager" # ğŸŒŸ å…³é”®ï¼šå¼ºåˆ¶å…³é—­ FlashAttnï¼Œè§£å†³ bprop æŠ¥é”™
    )

    # ================= 4. LoRA é…ç½® =================
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=4,              # LoRA ç§©
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"] # é’ˆå¯¹ Qwen çš„å…³é”®å±‚
    )
    model = get_peft_model(model, peft_config)
    
    # ğŸŒŸ æ˜¾å­˜ä¼˜åŒ–ï¼šå¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
    model.enable_input_require_grads()
    model.gradient_checkpointing_enable()

    if rank_id == 0:
        model.print_trainable_parameters()

    # ================= 5. æ•°æ®å¤„ç† =================
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset('json', data_files=DATA_FILE, split='train')

    def process_func(examples):
        """å°†æ•°æ®è½¬æ¢ä¸º Qwen çš„ ChatML æ ¼å¼ input_ids"""
        instructions = examples['instruction']
        inputs = examples['input']
        outputs = examples['output']
        
        full_texts = []
        for instr, inp, out in zip(instructions, inputs, outputs):
            # æ‰‹åŠ¨æ„å»º ChatML æ ¼å¼
            text = f"<|im_start|>system\n{instr}<|im_end|>\n<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n{out}<|im_end|>"
            full_texts.append(text)
            
        # Tokenize
        # max_length è®¾ä¸º 1024 æˆ– 2048 ä»¥èŠ‚çœæ˜¾å­˜
        tokenized = tokenizer(full_texts, padding="max_length", truncation=True, max_length=1024)
        
        # å°† input_ids å¤åˆ¶ç»™ labels (å…¨é‡è®¡ç®— Loss)
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    train_dataset = dataset.map(process_func, batched=True)

    # ================= 6. è®­ç»ƒå‚æ•° =================
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,   # 14B æ¨¡å‹å•å¡ Batch åªèƒ½è®¾ 1
        gradient_accumulation_steps=4,   # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆ Batch Size = 8 * GPUæ•°
        gradient_checkpointing=True,     # å¿…é¡»å¼€å¯
        
        # ğŸŒŸ å…³é”®ï¼šAscend 910B å¿…é¡»ç”¨ bf16ï¼Œä¸èƒ½ç”¨ fp16 (é¿å¼€ amp æŠ¥é”™)
        fp16=False,
        bf16=True,
        
        num_train_epochs=3,              # è®­ç»ƒè½®æ•°
        learning_rate=2e-4,              # LoRA å­¦ä¹ ç‡
        logging_steps=5,
        save_steps=100,
        save_total_limit=2,
        optim="adamw_torch",
        ddp_find_unused_parameters=False,
        report_to=[],                  # ä¸ä¸Šä¼  wandb
        per_device_eval_batch_size=1,  # è¯„ä¼°æ—¶ä¹Ÿç”¨å°batch
        eval_accumulation_steps=1,
        remove_unused_columns=True,  # è‡ªåŠ¨ç§»é™¤æ— ç”¨åˆ—
    )
    

    # ================= 7. å¼€å§‹è®­ç»ƒ =================
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
    )

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æƒé‡ (ä»…ä¸»å¡)
    if rank_id == 0:
        model.save_pretrained(OUTPUT_DIR)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼LoRA æƒé‡å·²ä¿å­˜è‡³ {OUTPUT_DIR}")

if __name__ == "__main__":
    train()