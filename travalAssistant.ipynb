{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82d3595-7aba-452e-b0e4-8f09bf4f3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'not support import any ops for now.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    }
   ],
   "source": [
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "## 下载并格式转换数据\n",
    "# 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', ms_dtype=mindspore.bfloat16, device_map=0)\n",
    "\n",
    "# 开启梯度检查点时，要执行该方法\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3151fb7-f134-40e0-9e5a-c27d8a60a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型和知识库...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3.10.14/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已移动到NPU\n",
      "模型和知识库加载完成！\n",
      "==================================================\n",
      "测试基本功能...\n",
      "问题: 你是谁？\n",
      "回答: 您好！我是北京旅游咨询专家，很高兴为您提供服务！\n",
      "==================================================\n",
      "\n",
      "问题: 你知道故宫的门票是多少钱吗？\n",
      "回答: 故宫门票的价格是门票价格是淡季120元，旺季140元。参观珍宝馆、钟表馆各需另外收费10元。\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "问题: 北京的恭王府好不好？\n",
      "回答: 北京的恭王府真的挺不错的，它是中国保存最完整、规模最大的皇家园林建筑群。\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "问题: 那你知道恭王府的开放时间是什么时候吗？\n",
      "回答: 开放时间是7:30-19:00（16:00停止入场，18:00停止入内，18:30停止对外使用）。\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "问题: 我看里面挺大的，你知道里面能逛多久吗？\n",
      "回答: 1小时 - 2小时吧，你知道门票多钱吗？\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "问题: 恭王府附近还有什么值得去的景点？\n",
      "回答: 景点有三座城楼、前门大街、天安门广场，都很好玩。\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "问题: 天安门广场需要门票吗？\n",
      "回答: 需要，你知道具体票价吗？\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import mindspore as ms\n",
    "from mindnlp import core \n",
    "from mindspore import nn\n",
    "from peft import PeftModel, LoraConfig\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class ModelServer:\n",
    "    def __init__(self, model_path, lora_path, knowledge_base_path,Model):\n",
    "        \"\"\"\n",
    "        初始化模型服务器\n",
    "        \n",
    "        Args:\n",
    "            model_path: 基础模型路径\n",
    "            lora_path: LoRA权重路径\n",
    "            knowledge_base_path: 知识库文件路径\n",
    "        \"\"\"\n",
    "        print(\"正在加载模型和知识库...\")\n",
    "        \n",
    "        # 加载tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 加载基础模型model = AutoModel.from_pretrained(model_path, mindspore_dtype=ms.bfloat16)\n",
    "        self.model = Model\n",
    "        \n",
    "        # 加载LoRA权重\n",
    "        self.model = PeftModel.from_pretrained(self.model, model_id=lora_path)\n",
    "        \n",
    "        # 移动到NPU设备（如果可用）\n",
    "        if ms.context.get_context('device_target') == 'Ascend':\n",
    "            self.model = self.model.npu()\n",
    "            print(\"模型已移动到NPU\")\n",
    "        else:\n",
    "            print(\"使用CPU运行\")\n",
    "        \n",
    "        # 加载RAG知识库\n",
    "        with open(knowledge_base_path, 'r', encoding='utf-8') as f:\n",
    "            self.knowledge_base = json.load(f)\n",
    "        \n",
    "        print(\"模型和知识库加载完成！\")\n",
    "    \n",
    "    # import jieba\n",
    "    # from sentence_transformers import SentenceTransformer, util\n",
    "    def retrieve_knowledge(self, query, top_k=3):\n",
    "        \"\"\"基于关键词的检索\"\"\" \n",
    "        query_words = set(query.lower().split()) \n",
    "        results = [] \n",
    "        \n",
    "        for item in self.knowledge_base: \n",
    "            content = item[\"content\"].lower() \n",
    "            score = len(query_words.intersection(set(content.split()))) \n",
    "            if score > 0: \n",
    "                results.append((score, item)) \n",
    "                \n",
    "            results.sort(key=lambda x: x[0], reverse=True) \n",
    "            return [item for _, item in results[:top_k]]\n",
    "#     def retrieve_knowledge(self, query, top_k=3):\n",
    "#         query_words = set(jieba.lcut(query.lower()))\n",
    "#         results = []\n",
    "\n",
    "#         for item in self.knowledge_base:\n",
    "#             content_words = set(jieba.lcut(item[\"content\"].lower()))\n",
    "#             score = len(query_words.intersection(content_words))\n",
    "#             if score > 0:\n",
    "#                 results.append((score, item))\n",
    "\n",
    "#         results.sort(key=lambda x: x[0], reverse=True)\n",
    "#         return [item for _, item in results[:top_k]]\n",
    "    \n",
    "    def generate_response(self, query, conversation_history=None, max_length=500):\n",
    "        \"\"\"生成回复\"\"\"\n",
    "        # 检索相关知识\n",
    "        knowledge = self.retrieve_knowledge(query)\n",
    "        \n",
    "        # 构建系统提示\n",
    "        system_prompt =\"\"\"你是一个专业的北京旅游咨询助手，请严格根据提供的知识回答问题。如果知识中没有提到，请回答“我在知识库中没有找到相关信息”。\n",
    "禁止编造。请基于以下信息回答问题：\\n\\n\"\"\"\n",
    "        \n",
    "        # 添加检索到的知识\n",
    "        knowledge_text = \"\"\n",
    "        for i, item in enumerate(knowledge):\n",
    "            knowledge_text += f\"[知识{i+1}] {item['content']}\\n\"\n",
    "        \n",
    "        # 添加对话历史\n",
    "        history_text = \"\"\n",
    "        if conversation_history:\n",
    "            history_text = \"\\n对话历史:\\n\" + \"\\n\".join(conversation_history[-4:]) + \"\\n\"\n",
    "        \n",
    "        # 构建完整的提示\n",
    "        full_prompt = f\"{system_prompt}{knowledge_text}{history_text}\\n用户问题: {query}\\n助手回答:\"\n",
    "        \n",
    "        # 构建模型输入\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个专业的北京旅游咨询助手\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "        \n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"ms\",\n",
    "            return_dict=True\n",
    "        ).to('cuda')\n",
    "        \n",
    "        # 生成参数\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": max_length,\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 50,\n",
    "            \"temperature\": 0.7,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        # 生成回复\n",
    "        with core.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "            # 只取生成的部分\n",
    "            generated_outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            response = self.tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response, knowledge\n",
    "    \n",
    "    def test_basic_function(self):\n",
    "        \"\"\"测试基本功能\"\"\"\n",
    "        print(\"测试基本功能...\")\n",
    "        \n",
    "        # 测试1: 简单问答\n",
    "        prompt = \"你是谁？\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你现在是一个专业的北京旅游咨询助手。\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"ms\",\n",
    "            return_dict=True\n",
    "        ).to('cuda')\n",
    "        \n",
    "        gen_kwargs = {\"max_length\": 1000, \"do_sample\": True, \"top_k\": 30}\n",
    "        \n",
    "        with core.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"问题: {prompt}\")\n",
    "        print(f\"回答: {response}\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    # 配置参数\n",
    "    model_path = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'#'./Qwen2.5-1.5B-Instruct' #'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'# './Qwen2.5-1.5B-Instruct'  # 或者使用本地路径\n",
    "    lora_path = './checkpoint-1300'  # 替换为您的LoRA权重路径\n",
    "    knowledge_base_path = './knowledge_base.json'  # 替换为您的知识库路径\n",
    "    \n",
    "    try:\n",
    "        # 初始化服务器\n",
    "        server = ModelServer(model_path, lora_path, knowledge_base_path,model)\n",
    "        \n",
    "        # 测试基本功能\n",
    "        print(\"=\" * 50)\n",
    "        server.test_basic_function()\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 测试RAG功能\n",
    "        test_questions = [\n",
    "            \"你知道故宫的门票是多少钱吗？\",\n",
    "            \"北京的恭王府好不好？\",\n",
    "            \"那你知道恭王府的开放时间是什么时候吗？\",\n",
    "            \"我看里面挺大的，你知道里面能逛多久吗？\",\n",
    "            \"恭王府附近还有什么值得去的景点？\",\n",
    "            \"天安门广场需要门票吗？\"\n",
    "        ]\n",
    "        \n",
    "        for question in test_questions:\n",
    "            print(f\"\\n问题: {question}\")\n",
    "            response, knowledge = server.generate_response(question)\n",
    "            print(f\"回答: {response}\")\n",
    "            \n",
    "            if knowledge:\n",
    "                print(\"参考知识:\")\n",
    "                for i, k in enumerate(knowledge):\n",
    "                    print(f\"  {i+1}. {k['content'][:100]}...\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"初始化失败: {e}\")\n",
    "        print(\"请检查:\")\n",
    "        print(\"1. 模型路径是否正确\")\n",
    "        print(\"2. LoRA权重路径是否正确\")\n",
    "        print(\"3. 知识库文件是否存在\")\n",
    "        print(\"4. 网络连接是否正常（如果使用在线模型）\")\n",
    "\n",
    "# 如果在Jupyter中运行，可以直接调用main()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ccbed-4459-4e31-9f35-6b5ea6055d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格8: 交互式对话\n",
    "def chat():\n",
    "    print(\"北京智能导游助手已启动！输入'退出'结束对话\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n您: \")\n",
    "        if user_input.lower() in ['退出', 'exit', 'quit']:\n",
    "            print(\"再见！\")\n",
    "            break\n",
    "        \n",
    "        response, knowledge = generate_response(user_input)\n",
    "        print(f\"助手: {response}\")\n",
    "        \n",
    "        # 可选：显示参考知识\n",
    "        if knowledge and input(\"显示参考知识？(y/n): \").lower() == 'y':\n",
    "            for i, k in enumerate(knowledge):\n",
    "                print(f\"  知识{i+1}: {k['content']}\")\n",
    "\n",
    "# 运行交互式对话\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
